NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
#### Prediction ####
DALYNEW <- c(seq(1,95,0.38))
len(lmENVH)
size(lmENVH)
help(size)
help("array")
dim(lmENVH)
dim(lmENVH)
dim(NEW)
lmENVH <- lm(EPI_data$ENVHEALTH ~ EPI_data$DALY + EPI_data$AIR_H + EPI_data$WATER_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
help(lm)
summary(EPI_data$ENVHEALTH)
data <- data.frame(Env = EPI_data$ENVHEALTH,
Daly = EPI_data$DALY,
Air_H = EPI_data$AIR_H,
Water_H = EPI_data$WATER_H)
data
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_H[is.na(data$Air_H)] <- mean(data$Air_H, na.rm = TRUE)
data$Env[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
data
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_H[is.na(data$Air_H)] <- mean(data$Air_H, na.rm = TRUE)
data$Water_H[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
data
lmENVH <- lm(data$ENVHEALTH ~ data$DALY + data$AIR_H + data$WATER_H )
boxplot(data$Env, data$Daly ,
data$Air_H , data$Water_H)
lmENVH <- lm(data$Env ~ data$Daly + data$Air_H + data$Water_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
cENV
#### Linear Model Using AIR_E ####
data <- data.frame(Env = EPI_data$ENVHEALTH,
Daly = EPI_data$DALY,
Air_E = EPI_data$AIR_E,
Water_H = EPI_data$WATER_E)
#Replacing all NA values with the mean of the column
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_E[is.na(data$Air_E)] <- mean(data$Air_E, na.rm = TRUE)
data$Water_H[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
boxplot(data$Env, data$Daly ,
data$Air_E , data$Water_H)
#### Linear Model ####
lmENVH <- lm(data$Env ~ data$Daly + data$Air_E + data$Water_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_ENEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_ENEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
cENV
#### Linear Model Using Climate as the dependent ####
data <- data.frame(Climate = EPI_data$CLIMATE,
Daly = EPI_data$DALY,
Air_H = EPI_data$AIR_H,
Water_H = EPI_data$WATER_E)
boxplot(data$Climate, data$Daly ,
data$Air_H , data$Water_H)
#### Linear Model ####
lmClimate <- lm(data$Climate ~ data$Daly + data$Air_H + data$Water_H )
lmClimate
summary(lmClimate)
cENCH <- coef(lmClimate)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_ENEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_ENEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmClimate, NEW, interval="prediction")
cENV <- predict(lmClimate, NEW, interval="confidence")
pENV
cENV
data <- read.csv("C:/Users/Naran/Downloads/dataset_multipleRegression.csv")
df <- data.frame(Unem = data$UNEM,
Hgrad = data$HGRAD,
Roll = data$ROLL)
sum(is.na(df$Unem))
sum(is.na(df$Hgrad))
sum(is.na(df$Roll))
df
[1,2]
(1,2)
(1 2 )
(1 2 4)
lmRoll <- lm(df$Roll ~ df$Unem + df$Hgrad)
pRoll <- predict(lmRoll, c(7,90000), interval="predict")
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
df <- data.frame(Unem = data$UNEM,
Hgrad = data$HGRAD,
Roll = data$ROLL,
Inc = data$INC)
sum(is.na(df$Inc))
lmRoll <- lm(Roll ~ Unem + Hgrad, data = df)
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
lmRoll <- lm(Roll ~ Unem + Hgrad, data = df)
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
conditions <- data.frame(c(7),c(90000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
conditions <- data.frame(Unem = c(7), Hgrad = c(90000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
df
lmRoll <- lm(Roll ~ Unem + Hgrad + Inc, data = df)
conditions <- data.frame(Unem = c(7), Hgrad = c(90000), Inc = (25000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
set.seed(12345)
par(mar = rep(0.2,4))
data_matrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data_matrix)[,nrow(data_matrix):1])
heatmap(data_matrix)
help(rbinom)
help(heatmap)
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i, ] <- data_matrix[i, ] + rep(c(0,3), each = 5)
}
}
heatmap(data_matrix)
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
image(1:10, 1:40, t(data_matrix)[, nrow(data_matrix):1])
heatmap(data_matrix)
hh <- hclust(dist(data_matrix))
data_matrix_ordered <= data_matrix[hh$order,]
par(mfrow = c(1,3))
hh <- hclust(dist(data_matrix))
data_matrix_ordered <- data_matrix[hh$order,]
par(mfrow = c(1,3))
image(t(data_matrix_ordered)[, nrow(data_matrix_ordered):1])
plot(rowMeans(data_matrix_ordered), 40:1, , xlab="The Row Mean", ylab = "Row", pch19)
plot(colMeans(data_matrix_ordered), 40:1, , xlab="Column", ylab = "Column Mean", pch19)
plot(rowMeans(data_matrix_ordered), 40:1, , xlab="The Row Mean", ylab = "Row", pch =19)
plot(colMeans(data_matrix_ordered), 40:1, , xlab="Column", ylab = "Column Mean", pch=19)
library(gdata)
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
bronx1<-bronx1[which(bronx1$GROSS.SQUARE.FEET!="0" & bronx1$LAND.SQUARE.FEET!="0" & bronx1$SALE.PRICE!="$0"),]
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
# read data in
nyt1<-read.csv("nyt1.csv")
# read data in
nyt1<-read.csv("nyt1.csv/")
# read data in
nyt1<-read.csv("./nyt1.csv/")
dir
path
# read data in
nyt1<-read.csv("nyt1.csv")
help(read/topic = )
help("read.csv")
# read data in
nyt1<-read.csv("nyt1.csv", header=TRUE)
# read data in
nyt1<-read.csv("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/Group2_Scripts/Lab2", header=TRUE)
require(kknn)
data(ionosphere)
# Creating the model using the training and testing set
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
(kknn)
require(kknn)
data(ionosphere)
#Splitting the data into a training/testing
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
# Creating the model using the training and testing set
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
# Training the model using the training set using a distance of 1
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
#Splitting the data into train and test
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")
[(iris.valid$Species != fit)+1])
install.packages("car")
require(car)
scatterplotMatrix(iris)
# and
scatterplotMatrix(swiss)
scatterplotMatrix(iris)
# and
scatterplotMatrix(swiss)
require(lattice)
super.sym <- trellis.par.get("superpose.symbol")
splom(~iris[1:4], groups = Species, data = iris,
panel = panel.superpose,
key = list(title = "Three Varieties of Iris",
columns = 3,
points = list(pch = super.sym$pch[1:3],
col = super.sym$col[1:3]),
text = list(c("Setosa", "Versicolor", "Virginica"))))
splom(~iris[1:3]|Species, data = iris,
layout=c(2,2), pscales = 0,
varnames = c("Sepal\nLength", "Sepal\nWidth", "Petal\nLength"),
page = function(...) {
ltext(x = seq(.6, .8, length.out = 4),
y = seq(.9, .6, length.out = 4),
labels = c("Three", "Varieties", "of", "Iris"),
cex = 2)
})
splom(~iris[1:3]|Species, data = iris,
layout=c(2,2), pscales = 0,
varnames = c("Sepal\nLength", "Sepal\nWidth", "Petal\nLength"),
page = function(...) {
ltext(x = seq(.6, .8, length.out = 4),
y = seq(.9, .6, length.out = 4),
labels = c("Three", "Varieties", "of", "Iris"),
cex = 2)
})
parallelplot(~iris[1:4] | Species, iris)
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))
## formula method
pairs(~ Fertility + Education + Catholic, data = swiss,
subset = Education < 20, main = "Swiss data, Education < 20")
pairs(USJudgeRatings)
## show only lower triangle (and suppress labeling for whatever reason):
pairs(USJudgeRatings, text.panel = NULL, upper.panel = NULL)
## put histograms on the diagonal
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(USJudgeRatings[1:5], panel = panel.smooth,
cex = 1.5, pch = 24, bg = "light blue",
diag.panel = panel.hist, cex.labels = 2, font.labels = 2)
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(USJudgeRatings, lower.panel = panel.smooth, upper.panel = panel.cor)
data("iris")
head(iris)
library(ggplot2)
library(e1071)
qplot(Petal.Length, Petal.Width, data=iris, color = species)
qplot(Petal.Length, Petal.Width, data=iris, color = Species)
library(ggplot2)
library(e1071)
svm_model1 <- svm(Species~., data=iris)
summary(svm_model1)
plot(svm_model1, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred1 <- predict(svm_model1, iris)
table1 <- table(Predicted = pred1, Actual = iris$Species)
table
table1
model1_accuracy = sum(diag(table1))/sum(table1)
model1_accuracy
model1_error <- 1 - model1_accuracy
model1_error
svm_model2 <- svm(Species~., data=iris, kernel = "linear")
summary(svm_model2)
summary(svm_model2)
plot(svm_model2, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred2 <- predict(svm_model2, iris)
table2 <- table(Predicted = pred2, Actual = iris$Species)
table2
model2_accuracy <- sum(diag(table2))/sum(table2)
model2_accuracy
model2_error <- 1 - model2_accuracy
model2_error
svm_model3 <- svm(Species~., data=iris, kernel = "polynomial")
summary(svm_model3)
plot(svm_model3, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=5))
plot(svm_model3, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred3 <- predict(svm_model3, iris)
table3 <- table(Predicted = pred3, Actual = iris$Species)
summary(svm_model1)
summary(svm_model2)
table3 <- table(Predicted = pred3, Actual = iris$Species)
table3
model3_accuracy <- sum(diag(table3))/sum(table3)
model3_accuracy
model3_error <- 1 - model3_accuracy
model3_error
help(svm)
coef(svm_model2)
coef(svm_model1)
setwd("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/UCI_Obesity_And_Diabetes/Diabetes datasets")
diabetic_data <- read.csv("diabetic_data.csv")
#Distribution of Age
age_distribution <- ggplot(diabetic_data, aes(x = age, fill = readmitted,label=..count..)) +
stat_count(binwidth=1)+
stat_count(binwidth=1)+
xlab("Age Range") +
ylab("Count") +
ggtitle("Age Distribution Inclduing Race")
age_distribution
library(ggplot2)
library(ggcorrplot)
#Distribution of Age
age_distribution <- ggplot(diabetic_data, aes(x = age, fill = readmitted,label=..count..)) +
stat_count(binwidth=1)+
stat_count(binwidth=1)+
xlab("Age Range") +
ylab("Count") +
ggtitle("Age Distribution Inclduing Race")
age_distribution
#Distribution of Age
age_distribution <- ggplot(diabetic_data, aes(x = age, fill = readmitted,label=..count..)) +
stat_count(binwidth=1)+
stat_count(binwidth=1)+
xlab("Age Range") +
ylab("Count") +
ggtitle("Age Distribution Inclduing Readmission")
age_distribution
plot(random_forest)
#### Comparing To A Decision Tree ####
library(caret)
library(randomForest)
rn_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
Train <- createDataPartition(rn_data$readmitted, p = 0.5, list = FALSE)
training <- logistic_data[ Train, ]
random_forest <- randomForest(readmitted~.,
data = training,
ntree = 50,
importance =TRUE,
na.action=na.exclude)
linear_regression_data <- data.frame(data.matrix(linear_regression_data))
setwd("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/UCI_Obesity_And_Diabetes/Diabetes datasets")
diabetic_data <- read.csv("diabetic_data.csv")
library(ggplot2)
library(ggcorrplot)
linear_regression_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
logistic_regression_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
#### Comparing To A Decision Tree ####
library(caret)
library(randomForest)
rn_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
Train <- createDataPartition(rn_data$readmitted, p = 0.5, list = FALSE)
training <- logistic_data[ Train, ]
testing <- logistic_data[ - Train, ]
set.seed(123)
random_forest <- randomForest(readmitted~.,
data = training,
ntree = 50,
importance =TRUE,
na.action=na.exclude)
logistic_data <- linear_regression_data[linear_regression_data$readmitted != 1, ]
logistic_data$readmitted <- as.factor(logistic_data$readmitted)
Train <- createDataPartition(logistic_data$readmitted, p = 0.7, list = FALSE)
training <- logistic_data[ Train, ]
rn_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
Train <- createDataPartition(rn_data$readmitted, p = 0.5, list = FALSE)
training <- logistic_data[ Train, ]
testing <- logistic_data[ - Train, ]
set.seed(123)
random_forest <- randomForest(readmitted~.,
data = training,
ntree = 50,
importance =TRUE,
na.action=na.exclude)
plot(random_forest)
# Load Obesity data #
setwd("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/UCI_Obesity_And_Diabetes/Obesity datasets")
obesity_data <- read.csv("ObesityDataSet_raw_and_data_sinthetic.csv")
library(caret)
library(randomForest)
Train <- createDataPartition(obesity_data$NObeyesdad, p = 0.7, list = FALSE)
training <- obesity_data[ Train, ]
testing <- obesity_data[ - Train, ]
set.seed(123)
random_forest <- randomForest(as.factor(NObeyesdad)~.,
data = training,
ntree = 50,
importance =TRUE,
na.action=na.exclude)
plot(random_forest)
linear_regression_data <- diabetic_data[-c(1:3,6,11:12, 19:21, 25:47 )]
colnames(linear_regression_data)
obesity_gender_cluster <- kmeans(new_obesity_data[,(3:4)], center=7, nstart=20)
catergorical_data_to_numeric <- data.frame(data.matrix(obesity_data[1:16]))
catergorical_data_to_numeric[1:16] <- as.data.frame(
lapply(catergorical_data_to_numeric[1:16],normalize))
new_obesity_data <- cbind(catergorical_data_to_numeric, obesity_data$NObeyesdad)
# Need to normalize the data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
catergorical_data_to_numeric[1:16] <- as.data.frame(
lapply(catergorical_data_to_numeric[1:16],normalize))
new_obesity_data <- cbind(catergorical_data_to_numeric, obesity_data$NObeyesdad)
library(cluster)
set.seed(123)
obesity_gender_cluster <- kmeans(new_obesity_data[,(3:4)], center=7, nstart=20)
obesity_gender_cluster
# Results #
table <- table(obesity_data$NObeyesdad, obesity_gender_cluster$cluster)
table
# 1   2
# Female 729 314
# Male   433 635
#Clustering Accuracy
sum(diag(table))/sum(table)
