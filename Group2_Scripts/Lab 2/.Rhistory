# read data in
nyt1<-read.csv("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/Group2_Scripts/Lab2", header=TRUE)
# read data in
nyt1<-read.csv("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/Group2_Scripts/Lab2", header=TRUE)
# read data in
nyt1<-read.csv("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/Group2_Scripts/Lab2/nyt1.csv", header=TRUE)
DataAnalytics2022_Alejandro_Naranjo
# read data in
nyt1<-read.csv("nyt1.csv", header=TRUE)
# eliminate zeros
nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Clicks>0 & nyt1$Age>0),]
## or could just have this: nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Age>0),]
nnyt1<-dim(nyt1)[1]
#90% to train
sampling.rate=0.9
#remainder to test
num.test.set.labels=nnyt1*(1.-sampling.rate)
#construct a random set of training indices (training)
training <-sample(1:nnyt1,sampling.rate*nnyt1, replace=FALSE)
#build the training set (train)
train<-subset(nyt1[training,],select=c(Age,Impressions))
#construct the remaining test indices (testing)
testing<-setdiff(1:nnyt1,training)
#define the test set
test<-subset(nyt1[testing,],select=c(Age,Impressions))
#construct labels for another variable (Gender) in the training set
cg<-nyt1$Gender[training]
#construct true labels the other variable in the test set
true.labels<-nyt1$Gender[testing]
#run the classifier, can change k
classif<-knn(train,test,cg,k=5)
#view the classifier
classif
#looks at attriburtes
attributes(.Last.value)
## or could just have this: nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Age>0),]
nnyt1<-dim(nyt1)[1]
# eliminate zeros
nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Clicks>0 & nyt1$Age>0),]
is.na(nyt1)
sum(is.na(nyt1))
summary(nyt1)
summary(nyt1)
sum(is.na(nyt1))
# eliminate zeros
nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Clicks>0 & nyt1$Age>0),]
## or could just have this: nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Age>0),]
nnyt1<-dim(nyt1)[1]
#90% to train
sampling.rate=0.9
#remainder to test
num.test.set.labels=nnyt1*(1.-sampling.rate)
#construct a random set of training indices (training)
training <-sample(1:nnyt1,sampling.rate*nnyt1, replace=FALSE)
#build the training set (train)
train<-subset(nyt1[training,],select=c(Age,Impressions))
#construct the remaining test indices (testing)
testing<-setdiff(1:nnyt1,training)
#define the test set
test<-subset(nyt1[testing,],select=c(Age,Impressions))
#construct labels for another variable (Gender) in the training set
cg<-nyt1$Gender[training]
#construct true labels the other variable in the test set
true.labels<-nyt1$Gender[testing]
#run the classifier, can change k
classif<-knn(train,test,cg,k=5)
#view the classifier
classif
#looks at attriburtes
attributes(.Last.value)
#view the classifier
classif
#run the classifier, can change k
classif<-knn(train,test,cg,k=5)
#run the classifier, can change k
library(kknn)
classif<-knn(train,test,cg,k=5)
#run the classifier, can change k
library("class")
classif<-knn(train,test,cg,k=5)
#run the classifier, can change k
library("class")
classif<-knn(train,test,cg,k=5)
#view the classifier
classif
#looks at attriburtes
attributes(.Last.value)
require(kknn)
data(ionosphere)
#Splitting into train and test
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
#Creating the model
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
#Looking at the model with the class data
table(ionosphere.valid$class, fit.kknn$fit)
#Re-training the model with newer values
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
iris.valid <- iris[val,]
# Using Iris
data(iris)
m <- dim(iris)[1]
# Sampling
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
# Spliting into train and test
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
# Creating the model
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
# Fiting the model
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")
[(iris.valid$Species != fit)+1])
require(mlbench)
data(HouseVotes84)
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
#Prediting
predict(model, HouseVotes84[1:10,-1])
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
require(mlbench)
data(HouseVotes84)
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
library(class)
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
#Prediting
predict(model, HouseVotes84[1:10,-1])
library(mlbench)
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
library(naivebayes)
library(naiveBayes)
library(e1071)
library(e1071)
#Creating the model and will be classifying
model <- naiveBayes(Class ~ ., data = HouseVotes84)
#Prediting
predict(model, HouseVotes84[1:10,-1])
predict(model, HouseVotes84[1:10,-1], type = "raw")
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
## Example of using a contingency table:
data(Titanic)
m <- naiveBayes(Survived ~ ., data = Titanic)
m
#Checking the results based off of category of Sex and Age
predict(m, as.data.frame(Titanic)[,1:3])
## Example with metric predictors:
data(iris)
m <- naiveBayes(Species ~ ., data = iris)
## alternatively:
m <- naiveBayes(iris[,-5], iris[,5])
m
table(predict(m, iris[,-5]), iris[,5])
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
require(mlbench)
data(HouseVotes84)
library(klaR)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
#Creating the class, we are trying to classify the type of voter
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
pred <- predict(model, HouseVotes84[,-1])
table(pred$class, HouseVotes84$Class)
#Predicting the type of model
table(pred$class, HouseVotes84$Class)
View(houseVotes84)
View(HouseVotes84)
# Josh Walters
install.packages('ElemStatLearn')
library('ElemStatLearn')
# Josh Walters
install.packages('ElemStatLearn')
library('ElemStatLearn')
# Josh Walters
install.packages('ElemStatLearn')
library('ElemStatLearn', lib.loc="C:/Users/Naran/AppData/Local/R/win-library/4.2")
library("klaR") # different from e1071 naivebayes - try it too!
library("caret")
data(spam, package="ElemStatLearn")
sub = sample(nrow(spam), floor(nrow(spam) * 0.9))
table(predict(m, iswiss[2:7], swiss[,2])
data(swiss)
sclass <- kmeans(swiss[2:7], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[2:7], swiss[,2])
table(predict(m, iswiss[2:7], swiss[,2])
data(swiss)
data(swiss)
sclass <- kmeans(swiss[2:7], 3)
View(swiss)
sclass <- kmeans(swiss[2:6], 3)
table(sclass$cluster, swiss[,2])
library(e1071)
m <- naiveBayes(swiss[2:7], swiss[,2])
data(swiss)
View(swiss)
sclass <- kmeans(swiss[2:6], 3)
table(sclass$cluster, swiss[,2])
library(e1071)
m <- naiveBayes(swiss[2:6], swiss[,2])
table(predict(m, iswiss[2:6], swiss[,2]))
table(predict(m, iswiss[2:6], swiss[,3]))
table(predict(m, iswiss[2:6], swiss[,1]))
require(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(Swiss_rpart) # try some different plot options
text(Swiss_rpart) # try some different text options
require(party)
treeSwiss<-ctree(Species ~ ., data=iris)
data("iris")
# Using the iris data set
treeSwiss<-ctree(Species ~ ., data=iris)
plot(treeSwiss)
cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0))
cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0))
treeFert<-ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
cforest(Fertility ~ Agriculture + Education + Catholic, data = swiss, controls=cforest_control(mtry=2, mincriterion=0))
library(tree)
tr <- tree(Species ~ ., data=iris)
tr
tr$frame
plot(tr)
text(tr)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
plot <- (fitK + main="Conditional Inference Tree for Kyphosis")
plot <- (fitK, main="Conditional Inference Tree for Kyphosis")
fitK <- ctree(Kyphosis ~ Age + Number + Start, data=kyphosis)
plot(fitK, main="Conditional Inference Tree for Kyphosis")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple")
require(kknn)
library(kknn)
data(iris)
# Splitting into Training and Testing datasets
iris.learn <- iris[-val,] 	# train
iris.valid <- iris[val,]	# test
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
# Training the KNN model
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
summary(iris.kknn)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$C)
head(iris.kknn$fitted.values)
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$C)
head(iris.kknn$fitted.values)
require(randomForest)
library(randomForest)
# Creating the randomforest Model
fitKF <- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)
print(fitKF) 	# view results
#We see we have an error rate of 20.99%
importance(fitKF) # importance of each predictor
#Using swiss data
fitSwiss <- randomForest(Fertility ~ Agriculture + Education + Catholic, data = swiss)
print(fitSwiss) # view results
# Regression Model
print(fitSwiss) # view results
importance(fitSwiss) # importance of each predictor
varImpPlot(fitSwiss)
plot(fitSwiss)
getTree(fitSwiss,1, labelVar=TRUE)
help(randomForest) # look at all the package contents and the randomForest method options
# look at rfcv - random forest cross-validation -
help(rfcv)
plot(fitSwiss)
getTree(fitSwiss,1, labelVar=TRUE)
help(randomForest) # look at all the package contents and the randomForest method options
# look at rfcv - random forest cross-validation -
help(rfcv)
# other data....
data(imports85)
View(imports85)
fit_import <- randomForest(Price ~ make + engineSize + bore + horsepower, data=imports85 )
fit_import <- randomForest(price ~ make + engineSize + bore + horsepower, data=imports85 )
df <- data.frame(imports85)
df <- df[!is.na]
df <- na.omit(df)
fit_import <- randomForest(price ~ make + engineSize + bore + horsepower, data=imports85 )
df <- data.frame(imports85)
df <- na.omit(df)
summary(df)
fit_import <- randomForest(price ~ make + engineSize + bore + horsepower, data=imports85 )
fit_import <- randomForest(make ~ engineSize + bore + horsepower, data=imports85 )
fit_import <- randomForest(make ~ engineSize + bore + horsepower, data=df)
df['make']
fit_import <- randomForest(make ~ engineSize + bore + horsepower, data=df)
require(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(Swiss_rpart) # try some different plot options
text(Swiss_rpart) # try some different text options
# Regression Tree Example
require(rpart)
# build the  tree
fitM <- rpart(Mileage~Price + Country + Reliability + Type, method="anova", data=cu.summary)
printcp(fitM) # display the results
plotcp(fitM)
summary(fitM)
# plot tree
plot(fitM, uniform=TRUE, main="Regression Tree for Mileage ")
text(fitM, use.n=TRUE, all=TRUE, cex=.8)
# plot tree
plot(fitM, uniform=TRUE, main="Regression Tree for Mileage ")
text(fitM, use.n=TRUE, all=TRUE, cex=.8)
# prune the tree
pfitM<- prune(fitM, cp=0.01160389) # from cptable??? adjust this to see the effect
# plot the pruned tree
plot(pfitM, uniform=TRUE, main="Pruned Regression Tree for Mileage")
text(pfitM, use.n=TRUE, all=TRUE, cex=.8)
# plot the pruned tree
plot(pfitM, uniform=TRUE, main="Pruned Regression Tree for Mileage")
text(pfitM, use.n=TRUE, all=TRUE, cex=.8)
post(pfitM, file = "ptree2.ps", title = "Pruned Regression Tree for Mileage")
rsq.rpart(fitM) # visualize cross-validation results
library(e1071)
library(rpart)
data(Glass, package="mlbench")
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred <- predict(rpart.model, testset[,-10], type = "class")
printcp(rpart.model)
plotcp(rpart.model)
rsq.rpart(rpart.model)
print(rpart.model)
plot(rpart.model,compress=TRUE)
text(rpart.model, use.n=TRUE)
plot(rpart.pred)
plot(rpart.model,compress=TRUE)
text(rpart.model, use.n=TRUE)
fitK <- rpart(Kyphosis ~ Age + Number + Start, method="class", data=kyphosis)
help(rpart)
fitK <- rpart(Kyphosis ~ Age + Number + Start, method="class", data=kyphosis)
printcp(fitK) # display the results
plotcp(fitK) # visualize cross-validation results
summary(fitK) # detailed summary of splits
# plot tree
plot(fitK, uniform=TRUE, main="Classification Tree for Kyphosis")
text(fitK, use.n=TRUE, all=TRUE, cex=.8)
# plot tree
plot(fitK, uniform=TRUE, main="Classification Tree for Kyphosis")
text(fitK, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree
post(fitK, file = "kyphosistree.ps", title = "Classification Tree for Kyphosis") # might need to convert to PDF (distill)
pfitK<- prune(fitK, cp=   fitK$cptable[which.min(fitK$cptable[,"xerror"]),"CP"])
plot(pfitK, uniform=TRUE, main="Pruned Classification Tree for Kyphosis")
text(pfitK, use.n=TRUE, all=TRUE, cex=.8)
# read data in
aba<-read.csv("abalone.csv")
# read data in
aba<-read.csv("abalone.csv")
naba<-dim(aba)[1]
#90% to train
sampling.rate=0.9
#remainder to test
num.test.set.labels=naba*(1.-sampling.rate)
#construct a random set of training indices (training)
training <-sample(1:naba,sampling.rate*naba, replace=FALSE)
#build the training set (train)
train<-subset(aba[training,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
#construct the remaining test indices (testing)
testing<-setdiff(1:naba,training)
#define the test set
test<-subset(aba[testing,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
#construct labels for another variable (Rings) in the training set
crings<-aba$Rings[training]
#construct true labels the other variable in the test set
true.labels<-aba$Rings[testing]
#run the classifier, can change k
classif<-knn(train,test,crings,k=5)
#view the classifier
classif
#run the classifier, can change k
library(class)
classif<-knn(train,test,crings,k=5)
#view the classifier
classif
#looks at attributes
attributes(.Last.value)
#now do bayes.
View(abalone)
#now do bayes.
View(aba)
model <- naiveBayes( aba~ ., data = aba)
model <- naiveBayes( Rings~ ., data = aba)
library(e1071)
model <- naiveBayes( Rings~ ., data = aba)
#now do bayes.
View(aba)
predict(model, aba[1:8])
predict(model, aba[1:8], type = "raw")
pred <- predict(model, aba[1:8])
#Checking the results
table(pred, aba$Rings)
model <- naiveBayes( Rings~ ., data = aba)
predict(model, aba[1:8])
predict(model, aba[1:8], type = "raw")
pred <- predict(model, aba[1:8])
#Checking the results
table(pred, aba$Rings)
